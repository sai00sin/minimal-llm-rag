llm/generate.py

# -*- coding: utf-8 -*-
"""
テキスト生成ユーティリティ（最小実装 / 学習用）

- 役割: 保存済みのミニ GPT 風モデル (`MiniGPT`) と語彙 (`WordTokenizer`) を読み込み、
  プロンプトからサンプリングで続きを生成する。
- 生成戦略: 温度付きソフトマックス + top-k 事前フィルタ + 簡易リピティションペナルティ + n-gram反復禁止。
- 前提:
    * `ckpts/tokenizer.json` に WordTokenizer を保存済み
    * `ckpts/llm.pt` に学習済みパラメータを保存済み（`state_dict`, `block_size`, `vocab_size` を含む）
- 想定用途: 実運用ではなく、「生成の基本的な流れ」を学ぶための参照実装。

主なハイパーパラメータ:
- `max_new_tokens` : 生成する最大トークン数。早期終了条件により短く終わることもある。
- `temperature`    : 温度。>0 で確率的サンプリング、0 で貪欲（argmax）。
- `repetition_penalty` : 既出トークンのロジットを割る（弱める）係数。>1 で反復を抑制。
- `top_k` : 上位 k 語以外の候補を事前に無効化（ロジットを極小へ）。
- `no_repeat_ngram` : 直近の (n-1) に候補を足した n-gram が過去に出ていたら避ける（最大 10 回までリトライ）。

処理の概観:
1) トークナイザ・モデルのロード
2) プロンプトを ID 列にエンコード
3) 反復: 直近 `block_size` をモデルに入力→次トークンのロジットを取得
4) ロジットにヒューリスティクス（反復ペナルティ, top-k, 温度）を適用しサンプル
5) EOS や改行2連など簡易終了条件で break
6) ID 列をデコードして返却
"""

import torch
from llm.model import MiniGPT
from llm.tokenizer_word import WordTokenizer


def _top_k_filter(logits: torch.Tensor, k: int = 40) -> torch.Tensor:
    """上位 k 以外のロジットを極小値に置換してサンプリング対象から外す。

    形状:
      - 入力: `logits` … `[batch, vocab]` または `[..., vocab]` を想定
      - 出力: 同形状

    実装メモ:
      - `torch.topk(logits, k)` で各デバイスごとに上位 k のしきい値（最小要素）を取り、
        それ未満を大きな負値（ここでは `-1e10`）へ置換する。
      - 温度ソフトマックス前に呼ぶことで、確率質量を上位 k に集中させる。
    """
    if k is None or k <= 0:
        # k 指定なし（= フィルタしない）場合はそのまま返す
        return logits

    # v: 上位 k 個の値（…の集合）、_ : そのインデックス（未使用）
    v, _ = torch.topk(logits, k)
    # しきい値（上位 k の中で最小の値）を最後の次元に合わせてブロードキャスト可能な形に
    thr = v[..., -1, None]

    # しきい値未満の位置を極小値に置換（= サンプリングでほぼ選ばれなくする）
    return torch.where(logits < thr, torch.full_like(logits, -1e10), logits)


@torch.no_grad()  # 生成時は勾配不要（速度とメモリ節約）
def generate_text(
    prompt: str,
    max_new_tokens: int = 80,
    temperature: float = 0.5,
    repetition_penalty: float = 1.2,
    top_k: int = 40,
    no_repeat_ngram: int = 3,
) -> str:
    """プロンプトから続きのテキストを生成して返す。

    Args:
        prompt: 生成の起点となるテキスト。
        max_new_tokens: 追加で生成する最大トークン数。
        temperature: サンプリング温度。0 だと argmax（決定的）、大きいほど多様。
        repetition_penalty: 既出トークンを不利にする係数 (>1 で抑制)。
        top_k: softmax 前に上位 k 以外を切り捨てる（高速化と品質のトレードオフ）。
        no_repeat_ngram: n-gram の完全反復を禁止する n（3 なら tri-gram を禁止）。

    Returns:
        生成後テキスト（`prompt` を含む）。
    """
    # --- 1) トークナイザ & チェックポイントの読み込み
    tok = WordTokenizer.load("ckpts/tokenizer.json")
    ckpt = torch.load("ckpts/llm.pt", map_location="cpu")

    # モデルをチェックポイントのメタ情報で初期化
    model = MiniGPT(vocab_size=ckpt["vocab_size"], block_size=ckpt["block_size"])
    model.load_state_dict(ckpt["state_dict"])  # 学習済み重みをロード
    model.eval()  # 推論モード（Dropout などを無効化）

    # プロンプトをトークン ID 配列に変換し、形状を `[batch=1, seq_len]` に
    idx = torch.tensor([tok.encode(prompt)], dtype=torch.long)

    # EOS（文終端）トークン ID（存在しない設計の場合もあるので None 許容）
    id_eos = tok.tok2id.get("<EOS>")

    # n-gram 反復チェック関数
    def violates(ids: torch.Tensor, cand: torch.Tensor, n: int) -> bool:
        """`ids` の末尾 (n-1) に `cand` を連結した n-gram が過去に出現していたら True。

        - `ids`: `[1, cur_len]` の ID 列（バッチ 1 固定前提の簡易実装）
        - `cand`: 次候補 ID（スカラーテンソル想定）
        - `n`: n-gram 長（1 以下なら無効）

        計算量は O(cur_len * n)。小モデル向けの簡易版。
        """
        if n <= 1 or ids.size(1) < n - 1:
            return False
        seq = ids[0].tolist()
        # 末尾 (n-1) + 候補 で n-gram を構成
        pat = seq[-(n - 1) :] + [int(cand)]
        # 過去に同じ n-gram が出ていたか走査
        for i in range(len(seq) - n + 1):
            if seq[i : i + n] == pat:
                return True
        return False

    # --- 2) 逐次サンプリングでトークンを積み増す
    for _ in range(max_new_tokens):
        # 入力は直近 block_size トークン（長文はウィンドウで切り詰め）
        logits, _ = model(idx[:, -model.block_size :])  # 出力形状: [B=1, T, V]
        logits = logits[:, -1, :]  # 直近トークンの次トークン用ロジット: [1, V]

        # 2-1) repetition penalty: これまで出たトークンのスコアを弱める
        #     ※ ここでは「割る」方式。`>1` で小さくなる。引き算や温度調整など別実装もある。
        for t in set(idx[0].tolist()):
            logits[0, t] /= repetition_penalty

        # 2-2) top-k フィルタ（上位 k 以外をほぼ無効化）。確率のしっぽを刈る。
        logits = _top_k_filter(logits, top_k)

        # 2-3) サンプリング or 貪欲選択
        if temperature > 0:
            # 温度付き softmax で確率化
            probs = torch.softmax(logits / temperature, dim=-1)

            # n-gram 禁止を満たすまで最大 10 回リトライ
            tries = 0
            while True:
                next_id = torch.multinomial(probs, num_samples=1)  # 形状 [1,1]
                if not violates(idx, next_id[0, 0], no_repeat_ngram) or tries > 10:
                    break
                # 禁止 n-gram に当たった候補の確率を 0 にして再正規化
                # 注意: 次の 1 行はインデクシングの簡易記法で、`probs[0, next_id[0,0]] = 0` と同義にするのが安全。
                probs[0, next_id] = 0  # 簡易/学習用（本番は要厳密化）
                probs = probs / probs.sum()
                tries += 1
        else:
            # 決定的生成（温度 0）: もっとも確率の高いトークンを選ぶ
            next_id = torch.argmax(logits, dim=-1, keepdim=True)  # 形状 [1,1]

        # 2-4) EOS で早期終了（トークナイザ設計に依存）
        if id_eos is not None and int(next_id) == id_eos:
            break

        # 2-5) 生成列に連結
        idx = torch.cat([idx, next_id], dim=1)

        # 2-6) 簡易な整形による終了判定（改行 2 連で“段落終わり”とみなす）
        out_txt = tok.decode(idx[0].tolist())
        if out_txt.endswith("\n\n"):
            break

    # --- 3) ID -> テキストへ戻して返却
    return tok.decode(idx[0].tolist())

---

llm/model.py

# llm/model.py
# -*- coding: utf-8 -*-
import math
import torch
import torch.nn as nn

class CausalSelfAttention(nn.Module):
    """
    因果（未来情報を見ない）自己注意層。
    - 入力: x ∈ R^{B,T,C} （B=バッチ, T=系列長, C=埋め込み次元）
    - 出力: y ∈ R^{B,T,C} （残差接続に足し込めるよう同次元）
    """
    def __init__(self, n_embd, n_head, block_size, dropout=0.1):
        super().__init__()
        # マルチヘッド前提: 各ヘッドの次元 head_dim = n_embd // n_head が整数である必要
        assert n_embd % n_head == 0
        self.n_head = n_head

        # Q, K, V の線形射影（C → C）。後でヘッド数に分割する
        self.key = nn.Linear(n_embd, n_embd)
        self.query = nn.Linear(n_embd, n_embd)
        self.value = nn.Linear(n_embd, n_embd)

        # 注意重みと出力に対するドロップアウト
        self.attn_drop = nn.Dropout(dropout)
        self.resid_drop = nn.Dropout(dropout)

        # ヘッドを結合したあとに元の埋め込み次元へ戻す射影
        self.proj = nn.Linear(n_embd, n_embd)

        # 因果マスク（上三角が -inf になるように下三角行列を保持）
        # register_buffer により state_dict には含まれるが学習パラメータではない
        # shape: (1,1,block_size,block_size) → ブロードキャストで (B,h,T,T) に適用
        self.register_buffer(
            "mask",
            torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)
        )

    def forward(self, x):
        B, T, C = x.size()  # B=batch, T=token数, C=埋め込み次元

        # Q,K,V を計算し、(B, T, C) → (B, h, T, head_dim) に並べ替え
        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B,h,T,dh)
        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B,h,T,dh)
        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B,h,T,dh)

        # スケールド・ドット積注意: (B,h,T,dh) @ (B,h,dh,T) → (B,h,T,T)
        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))

        # 因果マスク適用: 未来位置（上三角）を -inf にして softmax 後に 0 へ
        # T は実際の長さ（< block_size のケースにも対応）
        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float("-inf"))

        # 注意分布（行方向＝クエリ位置ごとの確率分布）
        att = torch.softmax(att, dim=-1)
        att = self.attn_drop(att)

        # 文脈化表現の合成: (B,h,T,T) @ (B,h,T,dh) → (B,h,T,dh)
        y = att @ v

        # ヘッド結合: (B,h,T,dh) → (B,T,C)
        y = y.transpose(1, 2).contiguous().view(B, T, C)

        # 出力線形 + ドロップアウト（残差は呼び出し元で実施）
        y = self.resid_drop(self.proj(y))
        return y


class Block(nn.Module):
    """
    Transformer の基本ブロック（Pre-LN 構成）
    - x = x + Attn(LN(x))
    - x = x + MLP(LN(x))
    Pre-LN は勾配の流れが安定しやすく、深い層でも学習しやすい。
    """
    def __init__(self, n_embd, n_head, block_size, dropout=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(n_embd)  # 注意サブ層の前に正規化
        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout)
        self.ln2 = nn.LayerNorm(n_embd)  # MLP サブ層の前に正規化

        # 位置ごとの FFN（拡張 → 活性化 → 縮小）
        # 4*n_embd は論文の一般的設定（容量を増やしつつ計算コストを抑える）
        self.mlp = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.GELU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        # 残差接続（Pre-LN）：正規化→サブ層→残差加算
        x = x + self.attn(self.ln1(x))  # 自己注意
        x = x + self.mlp(self.ln2(x))   # FFN
        return x


class MiniGPT(nn.Module):
    """
    最小の GPT 風言語モデル（トークン埋め込み + 位置埋め込み + Transformerブロック列）
    - 語彙サイズ: vocab_size
    - 出力: 各時刻での語彙分布ロジット（CrossEntropy で次トークン予測）
    """
    def __init__(self, vocab_size, n_layer=2, n_head=2, n_embd=128, block_size=128, dropout=0.1):
        super().__init__()
        self.block_size = block_size

        # 埋め込み:
        # - tok_emb: token ID → ベクトル（単語の意味表現）
        # - pos_emb: 位置 ID → ベクトル（系列内位置の情報）
        self.tok_emb = nn.Embedding(vocab_size, n_embd)
        self.pos_emb = nn.Embedding(block_size, n_embd)

        self.drop = nn.Dropout(dropout)

        # Transformer ブロックをスタック
        self.blocks = nn.ModuleList([Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])

        # 出力前の最終 LayerNorm（Pre-LN 構成でも最終に LN を入れるのが一般的）
        self.ln_f = nn.LayerNorm(n_embd)

        # 語彙次元に投影する最終線形層（bias なし）
        # （発展: tok_emb.weight と weight tying するとパラメータ削減・性能向上が見込める）
        self.head = nn.Linear(n_embd, vocab_size, bias=False)

    def forward(self, idx, targets=None):
        """
        idx:     入力トークンID列  shape: (B, T)
        targets: 教師ラベル（次トークンID） shape: (B, T) or None
        return:  (logits, loss)
                 - logits: (B, T, V)
                 - loss:   スカラー (targets が与えられた場合のみ)
        """
        B, T = idx.size()
        assert T <= self.block_size, "入力系列長 T が block_size を超えています。"

        # 0..T-1 の位置ID（全バッチ共通） shape: (T,)
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)

        # トークン埋め込み + 位置埋め込み（位置はブロードキャストで各バッチへ）
        # x: (B, T, C)
        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]
        x = self.drop(x)

        # Transformer ブロックを順に適用
        for bl in self.blocks:
            x = bl(x)

        # 最終正規化 + 語彙へ投影
        x = self.ln_f(x)
        logits = self.head(x)  # (B, T, V)

        loss = None
        if targets is not None:
            # CrossEntropyLoss は (N, C) と (N,) を想定するため平坦化
            # - logits.view(-1, V), targets.view(-1)
            # - パディングを含むバッチを扱う場合は ignore_index を設定するのが実務的
            loss = nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1)
            )
        return logits, loss

---

llm/tokenizer_word.py

# -*- coding: utf-8 -*-
from pathlib import Path
import json
from fugashi import Tagger

# ------------------------------------------------------------
# 特殊トークンは先頭に固定配置（インデックスも固定化される）
# <PAD>: 充填用（学習/推論で長さを揃えるときに使用）
# <BOS>: 文頭（Beginning Of Sequence）
# <EOS>: 文末（End Of Sequence）
# <UNK>: 語彙外トークン（未知語の代替）
# ※ 語彙サイズに対して常に4つが予約される前提
# ------------------------------------------------------------
SPECIALS = ["<PAD>", "<BOS>", "<EOS>", "<UNK>"]

class WordTokenizer:
    """
    MeCab（fugashi）で分かち書きし、語彙（token ↔ id）を管理する最小実装。

    設計思想:
      - fugashi.Tagger による形態素解析で「表層形」を採用（原形ではなく、出現文字列ベース）
      - build_from_text() で単一テキストから語彙を構築（頻度フィルタ・語彙上限あり）
      - encode()/decode() で token ID 列 <-> 文字列 を相互変換
      - 保存形式は JSON（{"vocab": [...] } のみ）

    想定用途:
      - 小規模コーパスでの「単語レベル」言語モデル実験
      - RAG の前処理など、単純な日本語分割が欲しいケース

    注意:
      - fugashi の辞書が必要。手軽に使うなら `unidic-lite` を pip 追加。
      - decode は特殊トークンを除去し、日本語のため空白なしで連結する仕様。
    """
    def __init__(self, vocab=None):
        # Tagger の初期化（辞書が見つからない場合は ImportError 相当で失敗）
        self.tagger = Tagger()
        if vocab:
            # 既存の語彙が与えられた場合: 逆引きマップも同時に構築
            self.id2tok = vocab                                  # id -> token（リスト）
            self.tok2id = {t: i for i, t in enumerate(vocab)}    # token -> id（辞書）
        else:
            # 語彙が未確定のとき（build_from_text で構築する想定）
            self.id2tok, self.tok2id = None, None

    def _wakati(self, s: str):
        """
        内部ユーティリティ: 文字列 s を「表層形」の列に分割して返す。
        - m.surface は実際にテキストに現れた形（活用や表記ゆれをそのまま保持）
        - 語彙は表層形ベースで構築されるため、正規化は別途方針次第
        """
        return [m.surface for m in self.tagger(s)]

    @staticmethod
    def build_from_text(text: str, min_freq: int = 1, max_vocab: int | None = None):
        """
        単一の生テキストから語彙を構築し、WordTokenizer を返す。
        - min_freq: 出現頻度の下限（これ未満は語彙に含めない）
        - max_vocab: 語彙サイズの上限（特殊トークン分を含めて抑制）
                     None の場合は無制限

        処理の流れ:
          1) fugashi で分かち → 出現頻度辞書を作成
          2) min_freq でフィルタ
          3) (頻度降順, 語順) で安定ソート（タイブレークの再現性確保）
          4) max_vocab が指定されていれば SPECIALS を除いた枠内に切り詰め
          5) SPECIALS を先頭に連結して最終 vocab を確定
        """
        t = WordTokenizer()
        freqs = {}
        # O(N) で頻度カウント（Nはトークン数）
        for w in t._wakati(text):
            freqs[w] = freqs.get(w, 0) + 1

        # 出現頻度しきい値でフィルタリング
        items = [(w, c) for w, c in freqs.items() if c >= min_freq]

        # 頻度の降順 → 語順（表層文字列の昇順）で安定化
        # 例: 出現回数が同じ語の並びが OS/辞書差で揺れないようにする
        items.sort(key=lambda x: (-x[1], x[0]))

        # max_vocab が指定されている場合、SPECIALS を差し引いた枠に制限
        if max_vocab:
            items = items[:max_vocab - len(SPECIALS)]

        # 語彙 = SPECIALS（固定） + 出現語（頻度順）
        vocab = SPECIALS + [w for w, _ in items]

        # 確定した語彙でトークナイザを初期化
        tok = WordTokenizer(vocab)
        return tok

    def encode(self, s: str, add_special=True):
        """
        文字列 s を token ID のリストに変換する。
        - add_special=True の場合:
            先頭に <BOS>、末尾に <EOS> を自動付与
          False の場合:
            生の列のみ（1トークン先予測タスクなどで都合が良い）

        語彙外トークンは <UNK> の ID にフォールバックする。
        """
        ids = []
        if add_special:
            ids.append(self.tok2id["<BOS>"])
        for w in self._wakati(s):
            i = self.tok2id.get(w, self.tok2id["<UNK>"])  # 未知語は <UNK>
            ids.append(i)
        if add_special:
            ids.append(self.tok2id["<EOS>"])
        return ids

    def decode(self, ids):
        """
        token ID の列を文字列に復元する。
        - SPECIALS は本文から除外（可視化や生成結果の後処理で都合が良い）
        - 日本語は基本的に単語間スペースを入れない方針（用途に応じて変更可）
        """
        # id -> token が未確定なケースの安全ガードも兼ねる
        specials = {self.tok2id.get(x) for x in SPECIALS if self.tok2id and x in self.tok2id}
        toks = [self.id2tok[i] for i in ids if i not in specials]
        # 日本語は空白連結しない（必要なら " ".join(toks) に変更）
        return "".join(toks)

    def save(self, path: str):
        """
        語彙を JSON で保存する（{"vocab": [...]}）。
        - 将来の互換性のため、必要最小限のフィールドのみ出力
        - 保存前にディレクトリを自動作成
        """
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump({"vocab": self.id2tok}, f, ensure_ascii=False, indent=2)

    @staticmethod
    def load(path: str):
        """
        save() で保存した JSON から語彙をロードしてトークナイザを復元。
        - 将来的にフィールド追加があっても、基本は "vocab" さえあれば復元可能
        """
        vocab = json.loads(Path(path).read_text(encoding="utf-8"))["vocab"]
        return WordTokenizer(vocab)

# ------------------------------------------------------------
# スクリプト単体実行時の簡易ビルド＆保存
# - data/corpus.txt から語彙を作成し ckpts/tokenizer.json に保存
# - vocab_size を標準出力
# ------------------------------------------------------------
if __name__ == "__main__":
    txt = Path("data/corpus.txt").read_text(encoding="utf-8")
    tok = WordTokenizer.build_from_text(txt, min_freq=1, max_vocab=20000)
    tok.save("ckpts/tokenizer.json")
    print("saved tokenizer.json | vocab_size=", len(tok.id2tok))

---

llm/train.py 

from pathlib import Path
import torch
from torch.utils.data import Dataset, DataLoader
from llm.tokenizer_word import WordTokenizer
from llm.model import MiniGPT

# ============================================================
# ハイパーパラメータ
# ------------------------------------------------------------
# BLOCK_SIZE:
#   1ステップでモデルに与えるトークン列の長さ（=文脈ウィンドウ長）。
#   x は長さ BLOCK_SIZE、y はその1トークン先を教師信号として持つ。
# BATCH_SIZE:
#   1回の更新で同時に処理するサンプル数。メモリに合わせて調整。
# LR:
#   学習率。AdamW の初期値として無難な 1e-3 を採用。
# EPOCHS:
#   データセット全体を何周するか。
# ============================================================
BLOCK_SIZE = 64
BATCH_SIZE = 32
LR = 1e-3
EPOCHS = 10

# ============================================================
# 1) コーパス読み込み
# ------------------------------------------------------------
# - 事前に data/corpus.txt を用意しておくこと。
# - ここでは「単語トークナイザ」でエンコードする前提なので、
#   改行や句読点の扱いは tokenizer 実装 (WordTokenizer) に依存。
# ============================================================
corpus = Path("data/corpus.txt").read_text(encoding="utf-8")

# ============================================================
# 2) Tokenizer 作成
# ------------------------------------------------------------
# - build_from_text: コーパスから語彙を作成するユーティリティ。
# - 実装側で未知語の扱い(UNK)や特殊トークン([PAD], [BOS], [EOS]など)
#   をどう設けるかは WordTokenizer 側の責務。
# ============================================================
tok = WordTokenizer.build_from_text(corpus)

# ============================================================
# 3) ids に変換
# ------------------------------------------------------------
# - テキストを整数ID列に変換。
# - add_special=False: ここでは学習サンプルを「1トークンずらし」で作るため、
#   追加の特殊トークンを自動で挿入しない方針（必要なら後で扱う）。
# - 例: 文章 [w1, w2, w3, w4] を ID にしたら
#   x=[w1,w2,w3], y=[w2,w3,w4] のように1つ先を予測させる。
# ============================================================
ids = tok.encode(corpus, add_special=False)

# ============================================================
# データセット定義
# ------------------------------------------------------------
# - (入力, 目標) のペアを「スライディングウィンドウ」で切り出す。
# - 入力 x: 連続した BLOCK_SIZE 個のトークンID
# - 目標 y: x の1トークン先（次トークン列）
#   => これは "teacher forcing" による次トークン予測設定。
# - __len__ は作れるペア数 = len(ids) - BLOCK_SIZE - 1
#   （y を1トークン先まで取るためにさらに -1）
# ============================================================
class TextDataset(Dataset):
    def __init__(self, ids, block_size):
        self.ids = ids
        self.block_size = block_size

    def __len__(self):
        # 作れる (x, y) の数。負にならないよう max(0, ...) でガード。
        return max(0, len(self.ids) - self.block_size - 1)

    def __getitem__(self, idx):
        # idx から連続する BLOCK_SIZE を入力 x に、
        # その1つ先（右に1シフト）を教師信号 y にする。
        # 例) ids: [0,1,2,3,4], BLOCK_SIZE=3, idx=0
        #     x=[0,1,2], y=[1,2,3]
        x = torch.tensor(self.ids[idx:idx + self.block_size])
        y = torch.tensor(self.ids[idx + 1:idx + self.block_size + 1])
        return x, y

# データセット生成
ds = TextDataset(ids, BLOCK_SIZE)

# データ数の最低チェック
n_samples = len(ds)
if n_samples == 0:
    # tokens が少なすぎてペアが作れない場合の明示エラー
    # 対処案: BLOCK_SIZE を下げる / コーパスを増やす / 特殊トークン追加
    raise ValueError(f"dataset too small: tokens={len(ids)}, BLOCK_SIZE={BLOCK_SIZE}")

# DataLoader はデータサイズ以下にバッチサイズを丸める（小規模コーパス対策）
batch_size = min(BATCH_SIZE, n_samples)

# ============================================================
# DataLoader
# ------------------------------------------------------------
# - shuffle=True: 各エポックでサンプル順をシャッフルして汎化を促す
# - drop_last=False: 端数バッチも学習に使う（小規模データで有利）
# - collate 関数はデフォルト（同次元 Tensor をバッチ化）
# ============================================================
dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=False)

print(f"[info] tokens={len(ids)} samples={n_samples} batch_size={batch_size}")

# ============================================================
# 4) モデル初期化
# ------------------------------------------------------------
# - vocab_size: 出力ロジットの最終次元（クラス数）
# - block_size: 入力の系列長
# - MiniGPT 側の forward 仕様:
#     logits, loss = model(x, y)
#   を想定（y を渡すと内部で CrossEntropy を計算して loss を返す作り）。
# - 最適化: AdamW（L2正則化を decoupled に扱う Adam の改良版）
# ============================================================
model = MiniGPT(vocab_size=len(tok.id2tok), block_size=BLOCK_SIZE)
opt = torch.optim.AdamW(model.parameters(), lr=LR)

# ============================================================
# 5) 学習ループ
# ------------------------------------------------------------
# - 典型的な PyTorch の学習手順：
#   1) 順伝播で logits と loss を取得
#   2) 勾配をゼロクリア (opt.zero_grad)
#   3) 逆伝播 (loss.backward)
#   4) パラメータ更新 (opt.step)
# - 集計:
#   total: そのエポックの合計損失
#   steps: 更新回数（=バッチ数）
#   ログは「平均損失」を表示
# - 形状の目安（実装により異なるが一般的には）：
#   x: [B, T]（B=batch, T=BLOCK_SIZE）
#   y: [B, T]
#   logits: [B, T, V]（V=vocab_size）
# ============================================================
for ep in range(EPOCHS):
    total = 0.0
    steps = 0
    for x, y in dl:
        logits, loss = model(x, y)  # y を渡すと内部で CE Loss を返す想定
        opt.zero_grad()             # 直前の勾配をリセット
        loss.backward()             # 逆伝播で勾配を蓄積
        opt.step()                  # 勾配に基づいてパラメータを更新
        total += loss.item()
        steps += 1
    print(f"epoch={ep+1} loss={total / max(1, steps):.4f}")

# ============================================================
# 6) 保存
# ------------------------------------------------------------
# - state_dict のみを保存（学習済み重みの最小セット）。
#   -> ローダブル: model.load_state_dict(torch.load(...))
# - トークナイザも別ファイルに保存（推論・再学習に必要）。
# - 注意：'ckpts' ディレクトリが無ければ作成すること。
#   例) Path('ckpts').mkdir(parents=True, exist_ok=True)
# ============================================================
torch.save(model.state_dict(), "ckpts/minigpt.pt")
tok.save("ckpts/tokenizer.json")

---

rag/build_index.py

# rag/build_index.py
# -*- coding: utf-8 -*-
from pathlib import Path
from joblib import dump
import numpy as np
from sklearn.preprocessing import normalize
from rag.vectorizer import build_tfidf
from rag.index import SimpleIndex

def read_docs(d):
    """
    指定ディレクトリ d 配下の .txt を読み込み、テキストと Path のリストを返す。
    - 返り値:
        texts: [str, ...]   各ファイルの本文
        paths: [Path, ...]  対応するファイルパス（後でメタ表示・デバッグに使う）
    """
    # 再現性のため glob 結果はソート（OS依存の順序ゆらぎを抑える）
    paths = sorted(Path(d).glob("*.txt"))
    # UTF-8 で素直に読む（社内Q&A前提のため BOM 等は想定しない）
    texts = [p.read_text(encoding="utf-8") for p in paths]
    return texts, paths

if __name__ == "__main__":
    # ------------------------------------------------------------
    # 1) 文書を読み込む
    # ------------------------------------------------------------
    # data/faq_docs/ に社内Q&Aの原稿 .txt を並べる想定
    texts, paths = read_docs("data/faq_docs")

    # ------------------------------------------------------------
    # 2) TF-IDF ベクトル化
    # ------------------------------------------------------------
    # - build_tfidf(texts) は以下を返す想定:
    #     X:     (N_docs, V) の TF-IDF 行列（numpy または scipy → ここでは toarray 済）
    #     vocab: 語彙（使わないが、解析・可視化時に役立つ）
    #     vec:   学習済み TfidfVectorizer（検索時のクエリを同じ前処理でベクトル化するため必須）
    X, vocab, vec = build_tfidf(texts)

    # ------------------------------------------------------------
    # 3) cos 類似の前処理として L2 正規化
    # ------------------------------------------------------------
    # - コサイン類似度 sim(q, d) = (q・d) / (||q|| * ||d||)
    #   文書側 D を L2 正規化しておくと、検索時は
    #   sim(q, d) = (q_norm・d_norm) で単なる内積になる。
    # - dtype は float32 に落とし、ディスク占有とメモリを削減。
    X = normalize(X, axis=1).astype(np.float32)

    # ------------------------------------------------------------
    # 4) 生成物の保存（ckpts/ に集約）
    # ------------------------------------------------------------
    # - tfidf.npz:   文書ベクトル群（X）
    # - tfidf_meta.json: 検索時に原文やパスを引くためのメタ情報
    # - tfidf_vectorizer.joblib: クエリ側のベクトル化で再利用
    Path("ckpts").mkdir(parents=True, exist_ok=True)
    np.savez("ckpts/tfidf.npz", X=X)

    # メタ情報:
    # - texts: 検索ヒットをそのまま返す・抜粋生成のため
    # - paths: どのファイルだったかを UI ログ/リンク表示に使う
    # - vec_path: ベクトライザの保存先（SimpleIndex がロード時に参照する前提）
    meta = {
        "texts": texts,
        "paths": [str(p) for p in paths],       # 後でパスを文字列で復元しやすくする
        "vec_path": "ckpts/tfidf_vectorizer.joblib",
    }
    (Path("ckpts")/"tfidf_meta.json").write_text(
        __import__("json").dumps(meta, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    # Vectorizer 本体は joblib で保存（scikit-learn の標準手法）
    dump(vec, "ckpts/tfidf_vectorizer.joblib")

    # ------------------------------------------------------------
    # 5) 早期検証（フォーマット/パス不整合の早期発見）
    # ------------------------------------------------------------
    # - SimpleIndex.load("ckpts") が例外なく動けば、最低限の読込経路は保証される。
    # - ここでは戻り値は捨てる（存在確認のみ）。
    _ = SimpleIndex.load("ckpts")

    print("saved ckpts/tfidf.npz, tfidf_meta.json, tfidf_vectorizer.joblib | docs=", len(texts))


---

rag/index.py

# -*- coding: utf-8 -*-
from __future__ import annotations
import json
from pathlib import Path
from typing import List, Tuple
import numpy as np
from joblib import load


class SimpleIndex:
    def __init__(self, X: np.ndarray, texts: List[str], vec, paths: List[str] | None = None):
        """
        シンプルな TF-IDF ベースの検索インデックス。
        - 役割: クエリ文字列を訓練時と同じベクトライザで TF-IDF 化し、
                文書行列 X とのコサイン類似度で上位文書を返す。

        Parameters
        ----------
        X :
            shape=(n_docs, vocab) の文書行列（TF-IDF）。各行は **L2正規化済み** を想定。
            dtype は float32 推奨（省メモリ・十分な精度）。
        texts :
            各ドキュメントの本文（またはスニペット）。検索結果に一緒に返す。
        vec :
            学習時のベクトライザ（例: sklearn.feature_extraction.text.TfidfVectorizer）。
            クエリを同じ前処理・語彙で数値化するために必須。
        paths :
            各ドキュメントのファイルパス（UIで出典を表示する用途）。省略可。
        """
        # X は float32 に統一（copy=False なので既に float32 ならコピーしない）
        self.X = X.astype(np.float32, copy=False)
        self.texts = texts
        self.vec = vec
        self.paths = paths or []

    @staticmethod
    def load(ckpt_dir: str | Path) -> "SimpleIndex":
        """
        build_index.py で保存した成果物（ckpts/配下）からインデックスを復元する。

        期待ファイル:
          - tfidf.npz                ... {"X": (n_docs, vocab)} の Numpy アーカイブ
          - tfidf_meta.json          ... {"texts": [...], "vec_path": "...", "paths": [...]}
          - tfidf_vectorizer.joblib  ... sklearn の TfidfVectorizer を joblib で保存したもの
        """
        ckpt_dir = Path(ckpt_dir)

        # 1) ベクトル本体のロード
        data = np.load(ckpt_dir / "tfidf.npz")
        X = data["X"]  # shape=(n_docs, vocab), L2正規化済み想定

        # 2) メタ情報（原文やファイルパス、ベクトライザパス）
        meta = json.loads((ckpt_dir / "tfidf_meta.json").read_text(encoding="utf-8"))
        texts = meta["texts"]
        vec_path = meta["vec_path"]
        paths = meta.get("paths", [])  # paths が無い保存形式にも耐性

        # 3) ベクトライザ（クエリ側で再利用）
        vec = load(vec_path)

        return SimpleIndex(X=X, texts=texts, vec=vec, paths=paths)

    def source_name(self, i: int) -> str:
        """
        検索結果で表示する出典名（ファイル名の stem）。
        paths を持っていない場合は "Doc{番号}" にフォールバック。
        """
        if self.paths and 0 <= i < len(self.paths):
            return Path(self.paths[i]).stem
        return f"Doc{i+1}"

    def query(self, question: str, top_k: int = 3) -> List[Tuple[int, float, str]]:
        """
        クエリ文字列に対する上位文書を返す。

        Returns
        -------
        List[Tuple[doc_index, score, text]]
            スコア（コサイン類似度）の降順に並んだタプルのリスト。
            - doc_index : ヒットした文書のインデックス（texts/paths へ参照可能）
            - score     : 類似度（-1.0〜1.0。ここでは TF-IDF なので 0〜1.0 付近が多い）
            - text      : 対応する本文（スニペット）

        NOTE
        ----
        - X はすでに L2 正規化済みを前提とし、クエリ側だけここで正規化する。
        - ベクトライザ vec は build_index 時のものと一致している必要がある。
        """
        # 1) クエリを TF-IDF 化（疎行列 → dense 1D ベクトルへ）
        #    .toarray() で (1, vocab) → [vocab] に変形し float32 化
        qv = self.vec.transform([question]).toarray().astype(np.float32)[0]  # shape=(vocab,)

        # 2) クエリの L2 正規化（ゼロ割り防止の微小量付き）
        denom = float(np.linalg.norm(qv) + 1e-12)
        qn = qv / denom  # shape=(vocab,)

        # 3) コサイン類似度の計算
        #    文書側は L2 正規化済み → cos(q, d) = (qn・d) = 行列積で OK
        #    scores: shape=(n_docs,)
        scores = self.X @ qn

        # 4) 上位 top_k を抽出（降順）
        #    np.argsort は O(n log n)。n_docs が非常に大きい場合は argpartition の検討余地あり。
        top_k = max(1, int(top_k))  # 不正値に対する最低1件ガード
        idx = np.argsort(-scores)[:top_k]

        # 5) 結果の整形（必要最小限: index, score, text）
        out: List[Tuple[int, float, str]] = [
            (int(i), float(scores[i]), self.texts[i]) for i in idx
        ]
        return out


---

rag/vectorizer.py

# rag/vectorizer.py
# -*- coding: utf-8 -*-
from __future__ import annotations
from typing import List, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

import re
from typing import List

# ※重要※
# 「学習時に使った前処理・分かち方・パラメータ」と
# 「推論時（ロード後）に使うそれら」は 100% 一致している必要がある。
# ここで定義した関数名や設定値を変えると、検索品質が大きく落ちるので要注意。

def _wakati(text: str) -> List[str]:
    """
    （現在は未使用だが将来の差し替え用）超簡易な日本語分かち。
    - 記号等をスペースへ置換 → 空白で split するだけ。
    - MeCab などの形態素解析へ切り替える場合の“足場”として残している。
    """
    # \w（英数アンダースコア）と日本語の主要ブロック以外をスペースに
    text = re.sub(r"[^\w一-龥ぁ-んァ-ヶｦ-ﾟ]+", " ", text)
    toks = [t for t in text.split() if t]
    return toks

def _preprocess(s: str) -> str:
    """
    ごく軽い正規化（文字数を増やさず無害化する範囲に限定）
    - 全角空白→半角空白
    - 改行コードの正規化（\r\n / \r → \n）
    """
    return s.replace("\u3000", " ").replace("\r\n", "\n").replace("\r", "\n")

def build_tfidf(docs: List[str]) -> Tuple[np.ndarray, list[str], TfidfVectorizer]:
    """
    文書集合 docs を TF-IDF ベクトル化する（日本語向けに“文字 n-gram”ベース）。
    設計ポイント:
      - analyzer='char_wb':
          単語境界（空白）内で文字 n-gram を作る。日本語では空白が少ないが、
          記号→空白の前処理により、文の“塊”を保った n-gram が得られやすい。
      - ngram_range=(2,4):
          2〜4 文字の n-gram。単語分割なしでも程よく表現力が出るバランス設定。
      - sublinear_tf=True:
          tf を 1 + log(tf) に圧縮し、長文の影響を緩和。
      - norm='l2':
          ベクトルを L2 正規化（コサイン類似度前提）。後段で再度 normalize しても整合。
    戻り値:
      X:    (n_docs, vocab) の dense 行列（float32）
      vocab: 使用語彙（n-gram）のリスト
      vec:   学習済み TfidfVectorizer（推論時のクエリ変換に必須）
    """
    # 1) 軽い前処理をかけ、学習・推論で同じ関数を共有することが重要
    docs_norm = [_preprocess(d) for d in docs]

    # 2) TF-IDF ベクトライザを定義
    vec = TfidfVectorizer(
        analyzer="char_wb",     # 文字 n-gram（単語境界単位）
        ngram_range=(2, 4),     # 2〜4 文字
        lowercase=False,        # 日本語の大文字小文字変換は基本不要
        min_df=1,               # すべての n-gram を対象（小規模コーパス前提）
        max_df=1.0,             # 上限なし（大規模なら 0.9 などに調整可）
        norm="l2",              # L2 正規化（疎→密にする前に内部で正規化）
        use_idf=True,           # IDF を使用
        sublinear_tf=True,      # tf を対数圧縮
    )

    # 3) 学習 & 変換（疎→dense、float32 にダウンクォート）
    #   ※ toarray() は密行列化するため文書数/語彙数が大きいとメモリを食う。
    #     必要に応じて疎のまま扱う設計に変更する。
    X = vec.fit_transform(docs_norm).astype(np.float32).toarray()

    # 4) 語彙リスト（モデル可視化やデバッグで役立つ）
    vocab = list(vec.get_feature_names_out())
    return X, vocab, vec

def encode_tfidf(vec: TfidfVectorizer, texts: List[str]) -> np.ndarray:
    """
    既存の（学習済み）ベクトライザ vec を用いて texts を TF-IDF へ変換するユーティリティ。
    - build_tfidf と同じ _preprocess を通すことで、学習/推論の整合性を担保。
    - 返り値は dense の float32（toarray() はメモリに注意）。
    """
    texts_norm = [_preprocess(t) for t in texts]
    return vec.transform(texts_norm).astype(np.float32).toarray()


---

app/api.py

# -*- coding: utf-8 -*-
from fastapi import FastAPI
from pydantic import BaseModel
from app.pipeline import QAPipeline

# ============================================================
# FastAPI アプリ定義
# ------------------------------------------------------------
# - title: ドキュメントUI（/docs, /redoc）や OpenAPI スキーマの表記に使われる
# - 本APIは「最小の RAG（Retrieve & Generate）デモ」用
# ============================================================
app = FastAPI(title="minimal-llm-rag")

# ============================================================
# パイプラインの常駐インスタンス
# ------------------------------------------------------------
# - QAPipeline() 側で、ckpts/ から TF-IDF インデックスや Vectorizer、
#   （必要なら）LLM をロードして、/ask で即問い合わせ可能にする設計。
# - プロセス起動時に一度だけロード → リクエスト毎に再利用（高速化）。
# - 注意: 大規模モデルや巨大インデックスの場合は起動時間・メモリ使用量に影響。
#   必要に応じて遅延ロードやバックグラウンドタスクでのプリウォームを検討。
# ============================================================
pipe = QAPipeline()

# ============================================================
# リクエストボディのスキーマ
# ------------------------------------------------------------
# - Pydantic(BaseModel) により型検証・バリデーションが自動化。
# - /docs でのインタラクティブUIや OpenAPI にも反映される。
#   例:
#   {
#     "query": "社外からVPNに接続する手順は？",
#     "top_k": 3
#   }
# ============================================================
class AskReq(BaseModel):
    query: str        # ユーザの自然文クエリ
    top_k: int = 3    # 上位何件のコンテキストを取得するか（既定=3）

# ============================================================
# エンドポイント: /ask
# ------------------------------------------------------------
# - 入力: AskReq（query, top_k）
# - 出力: pipe.ask(...) の戻り値（dictを想定）
#   例: {
#         "query": "...",
#         "results": [
#           {"rank": 1, "doc_id": 0, "score": 0.83, "text": "...", "source": "..."},
#           ...
#         ],
#         "answer": "..."
#       }
# - 注意:
#   - 戻り値のスキーマを固定したい場合は response_model=... を追加すると堅牢。
#   - 例外時の扱いを制御する場合は try/except で HTTPException を返す設計に拡張。
# ============================================================
@app.post("/ask")
def ask(req: AskReq):
    # QAPipeline 内部の処理（典型例）:
    # 1) ベクトル検索: req.query を TF-IDF 変換 → cos 類似で top_k を取得
    # 2) コンテキスト組み立て: ヒット文書の本文・出典をまとめる
    # 3) 生成（任意）: MiniGPT 等にプロンプトを渡し、コンテキスト制約付きで回答生成
    # 4) 辞書化して返却
    return pipe.ask(req.query, top_k=req.top_k)

---

app/pipeline.py

# -*- coding: utf-8 -*-
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple

from rag.index import SimpleIndex

# ------------------------------------------------------------
# 検索クエリ（日本語の質問）からノイズになりがちな語を除外するための簡易ストップワード。
# 例: 「〜の手順を教えてください」「〜とは？」などで、意味的なキーワード抽出を邪魔する語。
# 必要に応じてドメイン（社内用語など）に合わせて調整・拡張する。
# ------------------------------------------------------------
STOP_WORDS = {"どうやって", "何を", "手順", "ですか", "方法", "教えて", "してください", "とは"}

def _is_jp_char(ch: str) -> bool:
    """
    ざっくり「日本語っぽい1文字」かどうかを判定するヘルパー。
    - 対象: ひらがな / カタカナ / CJK 漢字 / よく使う記号（長音'ー'・中点'・'）
    - 目的: 英数や記号に混じる日本語部分のみを n-gram 抽出の対象にするため。
    """
    code = ord(ch)
    return (
        0x3040 <= code <= 0x309F  # ひらがな
        or 0x30A0 <= code <= 0x30FF  # カタカナ
        or 0x4E00 <= code <= 0x9FFF  # CJK 漢字
        or ch in "ー・"              # よく使う日本語記号
    )

def _jp_ngrams(text: str, n_low: int = 2, n_high: int = 4) -> List[str]:
    """
    入力文字列から「日本語部分のみ」を対象に 2〜4 文字の部分文字列（文字 n-gram）を作る。
    - 手順:
      1) 日本語文字のみ残し、非日本語はスペースへ（単語境界を作る）
      2) スペースで分割して「日本語の塊」を得る
      3) 各塊から n ∈ [n_low, n_high] の連続部分列を列挙
    - 目的: 形態素解析を使わずとも日本語の“キーワード断片”を拾うための簡便策。
    """
    s = "".join(ch if _is_jp_char(ch) else " " for ch in text)
    tokens = [t for t in s.split() if t]
    grams: List[str] = []
    for t in tokens:
        L = len(t)
        for n in range(n_low, n_high + 1):
            for i in range(0, max(0, L - n + 1)):
                grams.append(t[i:i+n])
    return grams

def _keywords_from_question(q: str) -> List[str]:
    """
    質問文 q から検索に使うキーワード候補を抽出する。
    - 句読点/記号 → スペース化 → 粗いトークン列（rough）を作る
    - ストップワードを除外
    - 英数字を含むトークン（VPN, Outlook, Duo などの製品名）を優先的に保持
    - さらに日本語の 2〜4 文字 n-gram を追加
    - 最後に順序を保った重複除去でまとめる
    """
    # 句読点・全角記号などを空白に。split で粗いトークン化。
    for ch in "？?！!。、，,.　\t\n":
        q = q.replace(ch, " ")
    rough = [t.strip() for t in q.split(" ") if t.strip()]
    # 汎用語（ストップワード）を間引く
    rough = [t for t in rough if t not in STOP_WORDS]

    # 英数を含むトークン（製品名・略語・型番などに効く）
    alnums = [t for t in rough if any(c.isalnum() for c in t)]

    # 日本語の主要キーワードを拾うため、n-gram で網を広げる
    grams = _jp_ngrams(q, 2, 4)

    # 順序保持の重複除去（先に alnums、次に grams を優先度順で連結）
    seen = set()
    keys: List[str] = []
    for t in alnums + grams:
        if t and t not in seen:
            seen.add(t)
            keys.append(t)
    return keys

def _pick_bullets_by_keywords(question: str, contexts: List[str]) -> List[str]:
    """
    上位文書の本文（contexts）から、質問に関係しそうな箇条書き（bullets）を抽出する。
    手順:
      1) contexts を行単位に分割し、ノイズ行を除去（空行・「テスト用」・token行など）
      2) 質問から抽出した keywords が含まれる行を順序保持で収集
      3) 抽出が少ない場合は、最上位文書の先頭から補完して最低数を満たす
    目的:
      - TF-IDF の上位文書は合っていても、本文は長いことが多い。
        キーワードと行単位のヒューリスティクスで“答えになりそうな箇所”をつまむ。
    """
    keys = _keywords_from_question(question)
    bullets: List[str] = []

    # 行リスト化（ノイズ・メタ行の除去）
    ctx_lines: List[List[str]] = []
    for c in contexts:
        lines = []
        for ln in c.splitlines():
            # 行頭の「・」「-」などの箇条書き記号を剥がして余白を整える
            s = ln.strip().lstrip("・-").strip()
            if not s:
                continue
            # 明示的に無視したい行のルール
            if s.startswith("テスト用") or "#token-" in s:
                continue
            lines.append(s)
        if lines:
            ctx_lines.append(lines)

    # 1) キーワード一致行を順序保持で抽出
    all_lines = [s for lines in ctx_lines for s in lines]
    for s in all_lines:
        if any(k in s for k in keys):
            if s not in bullets:
                bullets.append(s)

    # 2) 最低件数に満たない場合、最上位文書の先頭から補完（文脈の自然さを重視）
    MIN_BULLETS = 3
    if len(bullets) < MIN_BULLETS and ctx_lines:
        for s in ctx_lines[0]:  # 最上位文書の順序を尊重
            if s not in bullets:
                bullets.append(s)
            if len(bullets) >= MIN_BULLETS:
                break

    return bullets

def _format_answer(bullets: List[str], sources: List[str]) -> str:
    """
    箇条書き bullets と出典 sources から最終回答のテキストを整形する。
    - bullets が空なら「わかりません」を返す（出典はあれば併記）
    - sources は重複除去＆順序保持で "、" 連結
    出力例:
      ■ 回答
      - xxx
      - yyy
      ■ 出典: 001_入館とVPN, 002_勤怠と休暇
    """
    if not bullets:
        src = "、".join(dict.fromkeys(sources)) if sources else "Doc1"
        return "■ 回答\n- わかりません\n■ 出典: " + src
    body = "■ 回答\n" + "\n".join(f"- {b}" for b in bullets)
    src = "、".join(dict.fromkeys(sources)) if sources else "Doc1"
    return f"{body}\n■ 出典: {src}"

def _src_name(index: SimpleIndex, i: int) -> str:
    """
    SimpleIndex から i 番目ドキュメントの「出典名」を安全に取得する。
    - 優先: index.source_name(i) が提供されていればそれを使う
    - 次点: index.paths[i] の stem（拡張子なしファイル名）
    - 最後: "Doc{i}" フォールバック
    例外・属性未定義に対して頑健に作っている。
    """
    if hasattr(index, "source_name"):
        try:
            return index.source_name(i)  # type: ignore[attr-defined]
        except Exception:
            pass
    if hasattr(index, "paths") and getattr(index, "paths"):
        try:
            return Path(index.paths[i]).stem  # type: ignore[attr-defined]
        except Exception:
            pass
    return f"Doc{i}"

@dataclass
class QAPipeline:
    """
    「最小 RAG」用の問い合わせパイプライン。
    - 検索: SimpleIndex (TF-IDF + cos 類似) で上位文書を取得
    - 抽出: キーワードベースで行抽出して箇条書きを生成
    - 整形: 回答テキストにまとめ、出典も併記
    - パラメータ:
        thresh: 文書スコア（cos 類似）の下限しきい値。低すぎるヒットを弾く。
        max_k:  箇条書きの上限件数。冗長な回答を避ける。
    """
    index: SimpleIndex
    thresh: float = 0.12    # ← 少し緩め（スコアが低い環境や短文でも最低限拾うため）
    max_k: int = 5

    def __init__(self, ckpt_dir: str = "ckpts", thresh: float = 0.12, max_k: int = 5):
        # ckpts/ から TF-IDF 行列・メタ・ベクトライザをロードして検索器を準備
        self.index = SimpleIndex.load(ckpt_dir)
        self.thresh = thresh
        self.max_k = max_k

    def ask(self, question: str, top_k: int = 3) -> dict:
        """
        ユーザの質問に対して、検索→抽出→整形を行い、回答オブジェクトを返す。
        返り値の構造（例）:
        {
          "query": "社外からVPNに接続する手順は？",
          "contexts": [
            {"rank": 1, "score": 0.83, "text": "..."},
            {"rank": 2, "score": 0.61, "text": "..."},
            ...
          ],
          "answer": "■ 回答\n- ...\n- ...\n■ 出典: 001_入館とVPN"
        }
        """
        # 1) TF-IDF 検索（コサイン類似度の降順が返る想定）
        hits: List[Tuple[int, float, str]] = self.index.query(question, top_k=top_k)

        # 2) スコアしきい値でフィルタ（0.12 未満は弱ヒットとして除外）
        #    ただし何も残らなければ最上位1件だけは採用して「完全無回答」を避ける。
        filtered = [(i, s, t) for (i, s, t) in hits if s >= self.thresh]
        if not filtered:
            filtered = hits[:1]

        # 3) 行抽出用に本文だけ取り出す → キーワードに基づき箇条書きを生成
        contexts = [t for (_, _, t) in filtered]
        bullets = _pick_bullets_by_keywords(question, contexts)

        # 4) 箇条書きの件数上限（冗長防止）
        if self.max_k and len(bullets) > self.max_k:
            bullets = bullets[: self.max_k]

        # 5) 出典名の整形（重複を避けるのは _format_answer 側で実施）
        source_names = [_src_name(self.index, i) for (i, _, _) in filtered]
        answer = _format_answer(bullets, source_names)

        # 6) API レスポンス用の構造化（コンテキストは rank/score/text を返す）
        return {
            "query": question,
            "contexts": [
                {"rank": r + 1, "score": float(s), "text": t}
                for r, (i, s, t) in enumerate(filtered)
            ],
            "answer": answer,
        }

---

app/prompt.py 

from textwrap import dedent

def build_prompt(question: str, contexts: list[str]):
    """
    RAG の「生成（G）」側で LLM に渡すプロンプトを構築する。
    - system メッセージ相当の規約文（役割・出力フォーマット）と、
      user メッセージ相当の質問・コンテキストを 1 つの文字列にまとめて返す
      （シンプルさ重視の最小実装）。

    Parameters
    ----------
    question : str
        ユーザの自然文の質問。
    contexts : list[str]
        検索で得たコンテキスト文（上位文書や抜粋）。[Doc1], [Doc2], ... と
        番号を振って LLM に根拠の出典を参照させる。

    Returns
    -------
    str
        LLM へ渡すプロンプト全文（system の規約 + user の質問/文脈）。
    """
    # ------------------------------------------------------------
    # コンテキスト整形:
    #   [Doc1]\n<本文>\n\n[Doc2]\n<本文>... のように連結する。
    #   - enumerate(..., start=1) ではなく i+1 としているのは
    #     既存コードに合わせた最小改変のため（意味は同じ）。
    #   - [DocN] というラベルを回答の「出典: [Doc?]」で参照させる。
    # ------------------------------------------------------------
    ctx = "\n\n".join(f"[Doc{i+1}]\n{c}" for i, c in enumerate(contexts))

    # ------------------------------------------------------------
    # system（役割と厳格な出力フォーマットの指示）:
    #   - dedent: インデントを外して可読性と余計な空白混入を防ぐ。
    #   - 箇条書き・不明時の挙動・出典の付与・出力フォーマットを明示。
    #   - 「以下のドキュメントのみを根拠」→ コンテキスト外の幻覚抑制。
    # ------------------------------------------------------------
    system = dedent("""
        あなたは社内ヘルプデスクのアシスタントです。
        以下のドキュメントのみを根拠に、箇条書きで簡潔に回答してください。
        不明な点は「わかりません」と書いてください。
        最後に出典として [Doc1] のように参照を付けてください。
        出力は次の形式に厳密に従ってください。

        回答:
        - ...
        - ...
        出典: [Doc?]
    """).strip()

    # ------------------------------------------------------------
    # user（質問と文脈の提示）:
    #   - LLM に「出力のみ」を求めることで余計な説明・前置きを抑制。
    #   - コンテキストは上で整えた [DocN] 付きの塊をそのまま渡す。
    # ------------------------------------------------------------
    user = f"質問: {question}\n\nコンテキスト:\n{ctx}\n\n出力のみを書いてください。"

    # ------------------------------------------------------------
    # 最終プロンプト:
    #   一つの文字列として返す。使用するクライアントが
    #   system/user 分離をサポートしない前提の最小構成。
    #   （チャットAPIを使う場合は system/user を別メッセージとして渡す実装に分離可）
    # ------------------------------------------------------------
    return system + "\n\n" + user

---

上記のシーケンスを教えてください。
